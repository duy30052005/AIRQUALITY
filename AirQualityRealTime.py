import os
import requests
import pandas as pd
from sqlalchemy import create_engine, text
from urllib.parse import quote_plus
from flask import Flask
from apscheduler.schedulers.background import BackgroundScheduler
from datetime import datetime
import sys
import gc
import logging

# ==============================================================================
# üî• C·∫§U H√åNH LOGGING (QUAN TR·ªåNG CHO RENDER)
# ==============================================================================
# √âp Python ƒë·∫©y log ra ngay l·∫≠p t·ª©c (kh√¥ng buffer)
try:
    sys.stdout.reconfigure(line_buffering=True)
except (AttributeError, Exception):
    pass

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)s | %(message)s',
    handlers=[logging.StreamHandler(sys.stdout)]
)
logger = logging.getLogger()

# ==============================================================================
# 1. C·∫§U H√åNH K·∫æT N·ªêI DATABASE
# ==============================================================================
# Khuy·∫øn ngh·ªã: N√™n ƒë·∫∑t DB_PASSWORD trong Environment Variables c·ªßa Render
DB_PASS = os.getenv("DB_PASSWORD", "Duy@12345") 
DB_USER = "postgres.bkqhsxdynslfdtkcucij"
DB_HOST = "aws-1-ap-southeast-1.pooler.supabase.com"
DB_PORT = "6543"
DB_NAME = "postgres"

try:
    encoded_pass = quote_plus(DB_PASS)
    # pool_pre_ping=True gi√∫p t·ª± ƒë·ªông k·∫øt n·ªëi l·∫°i n·∫øu DB ng·∫Øt k·∫øt n·ªëi
    DB_URI = f"postgresql://{DB_USER}:{encoded_pass}@{DB_HOST}:{DB_PORT}/{DB_NAME}"
    engine = create_engine(DB_URI, pool_pre_ping=True)
    logger.info("‚úÖ K·∫øt n·ªëi Database th√†nh c√¥ng!")
except Exception as e:
    logger.error(f"‚ùå L·ªói c·∫•u h√¨nh DB: {e}")

# C·∫•u h√¨nh API & V·ªã tr√≠
OPENAQ_API_KEY = os.getenv("OPENAQ_API_KEY", "42eedf3f60d586732ed805ef7cc217bdb2c01bdaa34556e28a68093db6f08113")
LOCATION_ID_AQ = 4946812
SENSOR_MAP_AQ = {
    13502163: "co", 13502162: "no2", 13502148: "o3",
    13502153: "pm10", 13502151: "pm25", 13502157: "so2"
}
LAT, LON = 21.02, 105.85
LOCATION_KEY_WEATHER = 1 

# ==============================================================================
# 2. HELPER FUNCTIONS
# ==============================================================================
def ensure_dim_date(conn, unique_dates):
    """ƒê·∫£m b·∫£o c√°c ng√†y c√≥ trong Dim_Date ƒë·ªÉ tr√°nh l·ªói kh√≥a ngo·∫°i"""
    for d_key in unique_dates:
        try:
            d_key_int = int(d_key)
            exists = conn.execute(text(f'SELECT 1 FROM "Dim_Date" WHERE "DateKey" = {d_key_int}')).fetchone()
            if not exists:
                dt = datetime.strptime(str(d_key_int), '%Y%m%d')
                conn.execute(text(f"""
                    INSERT INTO "Dim_Date" ("DateKey", "FullDate", "Day", "Month", "Year", "DayOfWeek")
                    VALUES (:k, :fd, :d, :m, :y, :dow)
                """), {"k": d_key_int, "fd": dt.date(), "d": dt.day, "m": dt.month, "y": dt.year, "dow": dt.strftime('%A')})
                logger.info(f"üìÖ ƒê√£ t·∫°o ng√†y m·ªõi trong Dim_Date: {d_key_int}")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è L·ªói t·∫°o Dim_Date {d_key}: {e}")
            continue

# ==============================================================================
# 3. JOB 1: AIR QUALITY ETL (ƒê√É FIX L·ªñI X√ìA D·ªÆ LI·ªÜU)
# ==============================================================================
def run_air_quality_job():
    logger.info("üí® JOB 1: B·∫Øt ƒë·∫ßu l·∫•y d·ªØ li·ªáu Air Quality...")
    url = f"https://api.openaq.org/v3/locations/{LOCATION_ID_AQ}/latest"
    headers = {"X-API-Key": OPENAQ_API_KEY}
    
    try:
        response = requests.get(url, headers=headers, timeout=20)
        if response.status_code != 200:
            logger.error(f"‚ùå L·ªói API OpenAQ: {response.status_code} - {response.text}")
            return

        data = response.json().get('results', [])
        if not data:
            logger.warning("‚ö†Ô∏è Kh√¥ng c√≥ d·ªØ li·ªáu m·ªõi t·ª´ OpenAQ.")
            return

        processed_rows = []
        
        # L·∫•y Mapping t·ª´ DB
        with engine.connect() as conn:
            p_df = pd.read_sql(text('SELECT "ParameterName", "ParameterKey" FROM "Dim_Parameter"'), conn)
            l_df = pd.read_sql(text('SELECT "LocationID_Source", "LocationKey" FROM "Dim_Location"'), conn)
            
        param_db_map = dict(zip(p_df['ParameterName'], p_df['ParameterKey']))
        loc_db_map = dict(zip(l_df['LocationID_Source'], l_df['LocationKey']))
        loc_key = loc_db_map.get(LOCATION_ID_AQ)

        if not loc_key:
            logger.error(f"‚ùå Kh√¥ng t√¨m th·∫•y LocationKey cho ID {LOCATION_ID_AQ}")
            return

        # X·ª≠ l√Ω d·ªØ li·ªáu
        for item in data:
            sensor_id = item.get('sensorsId')
            param_name = SENSOR_MAP_AQ.get(sensor_id)
            if not param_name: continue
            
            # L·∫•y gi·ªù Local (quan tr·ªçng ƒë·ªÉ ƒë√∫ng m√∫i gi·ªù VN)
            dt_obj = pd.to_datetime(item.get('datetime', {}).get('local'))
            
            processed_rows.append({
                'DateKey': int(dt_obj.strftime('%Y%m%d')),
                'TimeKey': int(dt_obj.hour * 100 + dt_obj.minute),
                'LocationKey': loc_key,
                'ParameterKey': param_db_map.get(param_name),
                'SourceKey': 1,
                'Value': item.get('value')
            })
        
        if not processed_rows: return
        
        # DataFrame d·ªØ li·ªáu m·ªõi
        df_fact = pd.DataFrame(processed_rows).drop_duplicates(subset=['DateKey', 'TimeKey', 'LocationKey', 'ParameterKey'])
        unique_dates = df_fact['DateKey'].unique().tolist()
        date_str = ", ".join(str(x) for x in unique_dates)

        # --- PH·∫¶N S·ª¨A L·ªñI QUAN TR·ªåNG: CHECK TR√ôNG TR∆Ø·ªöC KHI INSERT ---
        with engine.connect() as conn:
            ensure_dim_date(conn, unique_dates)
            
            # 1. T√¨m d·ªØ li·ªáu ƒë√£ c√≥ trong DB
            query_check = text(f"""
                SELECT "DateKey", "TimeKey", "ParameterKey" 
                FROM "Fact_AirQuality" 
                WHERE "LocationKey" = {loc_key} AND "DateKey" IN ({date_str})
            """)
            existing_df = pd.read_sql(query_check, conn)
        
        # 2. L·ªçc b·ªè d√≤ng ƒë√£ t·ªìn t·∫°i
        if not existing_df.empty:
            existing_df['exists'] = True
            keys = ['DateKey', 'TimeKey', 'ParameterKey']
            # Left Join: Gi·ªØ l·∫°i b√™n tr√°i (M·ªõi), gh√©p v·ªõi b√™n ph·∫£i (C≈©)
            df_merged = pd.merge(df_fact, existing_df, on=keys, how='left')
            # Ch·ªâ l·∫•y d√≤ng m√† b√™n ph·∫£i kh√¥ng c√≥ d·ªØ li·ªáu (exists is NaN)
            df_final = df_merged[df_merged['exists'].isna()].drop(columns=['exists'])
        else:
            df_final = df_fact

        # 3. Insert d√≤ng m·ªõi
        if not df_final.empty:
            with engine.begin() as conn:
                df_final.to_sql('Fact_AirQuality', conn, if_exists='append', index=False)
                logger.info(f"‚úÖ ƒê√£ th√™m {len(df_final)} d√≤ng m·ªõi v√†o Fact_AirQuality.")
        else:
            logger.info("‚ö° D·ªØ li·ªáu ƒë√£ t·ªìn t·∫°i, kh√¥ng c·∫ßn c·∫≠p nh·∫≠t.")

    except Exception as e:
        logger.error(f"‚ùå L·ªói Job 1 (AQI): {e}")
    
    gc.collect()

# ==============================================================================
# 4. JOB 2: WEATHER ETL
# ==============================================================================
def run_weather_job():
    logger.info("‚òÄÔ∏è JOB 2: B·∫Øt ƒë·∫ßu l·∫•y d·ªØ li·ªáu Weather...")
    url = "https://api.open-meteo.com/v1/forecast"
    params = {
        "latitude": LAT, "longitude": LON,
        "hourly": "temperature_2m,relative_humidity_2m,wind_speed_10m,rain,surface_pressure",
        "timezone": "Asia/Bangkok", 
        "forecast_days": 3
    }

    try:
        response = requests.get(url, params=params, timeout=20)
        if response.status_code != 200:
             logger.error(f"‚ùå L·ªói API Weather: {response.status_code}")
             return

        data = response.json()
        df = pd.DataFrame(data['hourly'])
        df['time'] = pd.to_datetime(df['time'])
        
        # T·∫°o key
        df['DateKey'] = df['time'].dt.strftime('%Y%m%d').astype(int)
        df['TimeKey'] = df['time'].dt.hour * 100
        df['LocationKey'] = LOCATION_KEY_WEATHER
        
        mapping = {
            'temperature_2m': 'Temperature', 'relative_humidity_2m': 'Humidity', 
            'wind_speed_10m': 'WindSpeed', 'rain': 'Rain', 'surface_pressure': 'Pressure'
        }
        df.rename(columns=mapping, inplace=True)
        fact_df = df[['DateKey', 'TimeKey', 'LocationKey', 'Temperature', 'Humidity', 'WindSpeed', 'Rain', 'Pressure']]

        unique_dates = sorted(fact_df['DateKey'].unique())
        date_str = ", ".join(str(x) for x in unique_dates)

        with engine.begin() as conn:
            ensure_dim_date(conn, unique_dates)
            # V·ªõi d·ªØ li·ªáu d·ª± b√°o, vi·ªác ghi ƒë√® (Delete -> Insert) l√† ch·∫•p nh·∫≠n ƒë∆∞·ª£c ƒë·ªÉ c·∫≠p nh·∫≠t d·ª± b√°o m·ªõi nh·∫•t
            conn.execute(text(f'DELETE FROM "Fact_Weather" WHERE "LocationKey"={LOCATION_KEY_WEATHER} AND "DateKey" IN ({date_str})'))
            fact_df.to_sql('Fact_Weather', conn, if_exists='append', index=False)
            logger.info(f"‚úÖ ƒê√£ c·∫≠p nh·∫≠t {len(fact_df)} d√≤ng d·ª± b√°o th·ªùi ti·∫øt.")
            
    except Exception as e:
        logger.error(f"‚ùå L·ªói Job 2 (Weather): {e}")
    gc.collect()

# ==============================================================================
# 5. SERVER & SCHEDULER
# ==============================================================================
app = Flask(__name__)

@app.route('/')
def index():
    return f"üåç AI Data Service Active. Time: {datetime.now()}"

# C·∫•u h√¨nh Scheduler
scheduler = BackgroundScheduler()
# Ch·∫°y Job 1 m·ªói 15 ph√∫t (ƒë·ªÉ tr√°nh spam API v√† ph√π h·ª£p v·ªõi t·∫ßn su·∫•t sensor)
scheduler.add_job(func=run_air_quality_job, trigger="interval", minutes=15)
# Ch·∫°y Job 2 m·ªói 60 ph√∫t
scheduler.add_job(func=run_weather_job, trigger="interval", minutes=60)
scheduler.start()

if __name__ == "__main__":
    logger.info("‚ö° Kh·ªüi ƒë·ªông h·ªá th·ªëng...")
    
    # Ch·∫°y ngay 1 l·∫ßn khi kh·ªüi ƒë·ªông ƒë·ªÉ c√≥ d·ªØ li·ªáu
    run_air_quality_job()
    run_weather_job()
    
    port = int(os.environ.get("PORT", 5000))
    # use_reloader=False l√† b·∫Øt bu·ªôc v·ªõi APScheduler
    app.run(host='0.0.0.0', port=port, use_reloader=False)
